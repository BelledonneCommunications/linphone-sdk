import re
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import librosa
import matplotlib.pyplot as plt
from mfcc_comparison import mfcc_analysis

class AudioAnalysisNoiseSuppression:
    """
    This class handle the analysis of audio data generated by Noise Suppressor filtering.
    """

    def __init__(self, speech, noisy_speech, clean, model, test_suite_name):
        """
        Initialization of the class.
        :param speech: AudioSignal instance for speech.
        :param noisy_speech: AudioSignal instance for noisy speech.
        :param clean: AudioSignal instance for output of Noise Suppressor filter.
        """

        self.speech = speech
        self.noisy_speech = noisy_speech
        self.clean = clean
        self.model = model
        self.test_suite_name = test_suite_name
        self.log_file = ""
        self.sampling_rate_hz = 0
        self.energy_in_silence = 0
        self.energy_in_silence_with_noise = 0
        self.similarity_in_speech = 0
        self.msticker_late_ms = []
        self.maxpos = 0
        self.test_passed = 0
        self.asserts = 0
        self.total_time_s = 0
        self.silence_mask = None
        self.start_analysis_time_ms = 0
        self.start_sample = 0
        self.distance_mfcc = 0
        self.similarity_mfcc = 0
        self.filter_stats = np.zeros(6)
        self.noise_gain = 1.
        self.snr_dB = 0.

    def get_results(self, log_file):
        """
        Extracts the results of the AEC filtering during a test from mediastreamer2 test suite Noise Suppression.
        :param log_file: full file name of the log file.
        """

        self.log_file = log_file
        metrics_lines = []
        float_pattern = r'\d+\.\d+|\d+'
        with open(log_file) as f:
            contents = f.readlines()
            for i, line in enumerate(contents):

                if "Noise added with gain" in line:
                    match = re.search(r'([0-9]+\.[0-9]+)', line.split("Noise added with gain = ")[-1])
                    if match:
                        self.noise_gain = float(match.group(1))

                if "[Noise suppressor] set rate" in line:
                    self.sampling_rate_hz = int(line.split("set rate ")[-1].split(" Hz")[0])
                    print(f"sampling rate found is {self.sampling_rate_hz} Hz")

                if "chunk - max cross-correlation obtained at position " in line and self.maxpos == 0:
                    match = re.search(r'\[(-?\d+)\]',
                                      line.split("obtained at position ")[-1].split(", similarity factor")[0])
                    if match:
                        self.maxpos = int(match.group(1))

                if " similarity in talk" in line:
                    match = re.search(r"([0-9]+\.[0-9]+)", line.split("similarity in talk = ")[-1].split(" - min")[0])
                    if match:
                        self.similarity_in_speech = float(match.group(1).replace(",", "."))

                if "energy in silence = " in line:
                    match = re.search(r"([0-9]+\.[0-9]+)", line.split("energy in silence = ")[-1].split(" - max =")[0])
                    if match:
                        self.energy_in_silence = float(match.group(1).replace(",", "."))

                if "energy in silence of noisy audio = " in line:
                    match = re.search(r"([0-9]+\.[0-9]+)", line.split("energy in silence of noisy audio = ")[-1].split(", the remainig energy")[0])
                    if match:
                        self.energy_in_silence_with_noise = float(match.group(1).replace(",", "."))

                if "Tester MSTicker: We are late of" in line:
                    late_str = line.split("Tester MSTicker: We are late of ")[-1].split(" miliseconds.")[0]
                    self.msticker_late_ms.append(int(late_str))

                if "FILTER USAGE STATISTICS" in line:
                    if self.model == "mswebrtcns":
                        filter_name = "MSWebRTCNoiseSuppressor"
                    elif self.model == "rnnoise":
                        filter_name = "MSNoiseSuppressor"
                    k = i + 4
                    if k < len(contents):
                        stats_line = contents[k]
                        while k < len(contents) and filter_name not in stats_line and "=======================" not in stats_line:
                            stats_line = contents[k]
                            k = k + 1
                        if filter_name in stats_line:
                            self.filter_stats = np.array([float(num) for num in re.findall(float_pattern, stats_line.split(filter_name)[1])])

                if "Suite [Noise suppression] Test [" in line:
                    if "passed in" in line:
                        self.test_passed = 1
                    elif "failed in" in line:
                        self.test_passed = 0
                    total_time_str = line.split("in ")[-1].split(" secs")[0]
                    match = re.search(r'([0-9]+\.[0-9]+)', total_time_str)
                    if match:
                        self.total_time_s = float(match.group(1))

                if "Run Summary:    Type  Total    Ran Passed Failed Inactive" in line:
                    k = i + 3
                    if k < len(contents):
                        if "asserts" in contents[k]:
                            integers = [int(num) for num in contents[k].split() if num.isdigit()]
                            self.asserts = integers[3]

    def print(self):
        """
        Print the results of the AEC test.
        """

        if len(self.msticker_late_ms) > 0:
            print(f"MSticker late: \t{self.msticker_late_ms} ms")
        if self.clean is not None:
            print(
                f"maxpos: \t\t\t\t{self.maxpos} samples\n\t\t\t\t\t\t{self.maxpos * 1000. / self.clean.sample_rate_hz:0.0f} ms")
        else:
            print(
                f"maxpos: \t\t\t\t{self.maxpos} samples\n\t\t\t\t\t\t{self.maxpos * 1000. / self.sampling_rate_hz:0.0f} ms")

        print(f"energy in silence of clean audio: \t\t{self.energy_in_silence:0.2f}")
        print(f"energy in silence of noisy audio: \t\t{self.energy_in_silence_with_noise:0.2f}")
        print(f"similarity in speech: \t{self.similarity_in_speech:0.3f}")
        print(f"           with MFCC: \t{self.similarity_mfcc:0.3f}")
        if (self.snr_dB != 0.):
            print(f"SNR                 : \t{self.snr_dB:0.2f} dB")

        if self.filter_stats is not None:
            print(f"filter usage stats: \tcount\t\t\t{self.filter_stats[0]:0.0f}")
            print(f"\t\t\t\t\t\ttime\tmin\t\t{self.filter_stats[1]:0.2f} ms")
            print(f"\t\t\t\t\t\t\t\tmean\t{self.filter_stats[2]:0.2f} ms")
            print(f"\t\t\t\t\t\t\t\tmax\t\t{self.filter_stats[3]:0.2f} ms")
            print(f"\t\t\t\t\t\t\t\tstd\t\t{self.filter_stats[4]:0.2f} ms")
            print(f"\t\t\t\t\t\tCPU usage\t\t{self.filter_stats[5]:0.2f} %")

        print(f"test passed: \t\t\t{self.test_passed}")
        print(f"total time: \t\t\t{self.total_time_s:0.1f} s")

    @staticmethod
    def compute_correlation(audio_ref, audio_test):
        """
        Return the maximal cross-correlation and its position (in samples) between two audio.
        :param audio_ref: reference audio data
        :param audio_test: tested audio data
        :return: position of the maximal cross-correlation and maximal cross-correlation
        """

        max_length = max(len(audio_ref), len(audio_test))
        padded_ref = np.pad(np.abs(audio_ref), (0, max_length - len(audio_ref)))
        padded_test = np.pad(np.abs(audio_test), (0, max_length - len(audio_test)))
        correlation = np.correlate(padded_ref, padded_test, mode='full')
        max_pos_samples = np.argmax(correlation) - max_length + 1
        corr = max(correlation)
        if len(audio_ref) > len(audio_test):
            max_pos_samples = -1 * max_pos_samples

        return max_pos_samples, corr

    def align_on_speech(self):
        """
        Align the noisy audio and the filter output audio with reference audio on the maximal correlation.
        """
        print("-- Align clean audio on initial speech --")
        self.align_signal_on_reference(self.speech, self.clean)

        # self.plot_audio(self.clean, "clean audio")

        print("-- Align noisy speech on initial speech --")
        self.align_signal_on_reference(self.speech, self.noisy_speech)

        max_size = min([self.speech.aligned_data.size, self.noisy_speech.aligned_data.size, self.clean.aligned_data.size])
        self.speech.aligned_data = self.speech.aligned_data[:max_size]
        self.noisy_speech.aligned_data = self.noisy_speech.aligned_data[:max_size]
        self.clean.aligned_data = self.clean.aligned_data[:max_size]

        # output_path = "/home/flore/Dev/Linphone-sdk/submodules_removed/linphone-sdk/out/build/default-ninja-no-sanitizer/run_0/"
        # self.speech.write_aligned_in_file(output_path + "aligned_speech.wav")
        # self.noisy_speech.write_aligned_in_file(output_path + "aligned_noisy_speech.wav")
        # self.clean.write_aligned_in_file(output_path + "aligned_clean.wav")


    def plot_audio(self, audio, fig_title=""):
        """Plot the audio and return the plotly figure."""

        sample_duration_s = 1. / audio.sample_rate_hz
        signal_name = ["full audio", "aligned"]
        n_rows = len(signal_name)
        fig = make_subplots(rows=n_rows, cols=1, shared_xaxes=True)
        for i, audio_signal in enumerate([audio.data, audio.aligned_data]):
            if audio_signal is not None:
                timestamp = sample_duration_s * np.arange(audio_signal.size)
                signal = audio_signal / np.max(np.abs(audio_signal))
                fig.add_trace(go.Scatter(x=timestamp, y=signal, mode='lines', line={'width': 1},
                                         name=f"{signal_name[i]}"), row=i + 1, col=1)
            fig.update_yaxes(title_text=f'{signal_name[i]}', range=[-1., 1.], row=i + 1, col=1)
        fig.update_layout(title=fig_title)
        fig.update_xaxes(title_text='Time')
        fig.show()

        return fig

    def align_signal_on_reference(self, reference, audio, n=10):
        """
        Align the audio with the reference on the maximal correlation. The comparison is made on maximum n seconds. Once
         the shift is found, the aligned signals are created in reference.aligned_data and audio.aligned_data, from
         sample self.start_sample. The signal audio.aligned_data is created by shifting audio.data on left or right.
         Some 0 are added at start if needed.
        """

        stop_sample = self.start_sample + int(n * audio.sample_rate_hz)
        stop_sample = min(reference.data.size, stop_sample)
        stop_sample = min(audio.data.size, stop_sample)

        if reference.data is not None and audio.data is not None:

            max_pos_samples, corr = self.compute_correlation(reference.data[self.start_sample:stop_sample],
                                                             audio.data[self.start_sample:stop_sample])
            sample_duration_s = audio.sample_duration_s
            print(
                f"maximum correlation found at {max_pos_samples} samples {sample_duration_s * max_pos_samples * 1000:1.0f} ms with value {corr:1.1f}")

            reference.aligned_data = reference.data[self.start_sample:]
            if max_pos_samples is not None:
                if max_pos_samples < 0:
                    audio.aligned_data = audio.data[self.start_sample - max_pos_samples:]
                elif max_pos_samples > 0:
                    audio.aligned_data = np.concatenate((np.zeros((max_pos_samples)), audio.data))
                    audio.aligned_data = audio.data[self.start_sample:]
                else:
                    audio.aligned_data = audio.data[self.start_sample:]


        def align_on_reference(self, reference, audio, n=10, reference_aligned=False):
            """
            Align the audio with the reference on the maximal correlation. The comparison is made on maximum n seconds.
            """

            stop_sample = self.start_sample + int(n * audio.sample_rate_hz)
            stop_sample = min(reference.data.size, stop_sample)
            stop_sample = min(audio.data.size, stop_sample)

            if reference.data is not None and audio.data is not None:

                max_pos_samples, corr = self.compute_correlation(reference.data[self.start_sample:stop_sample],
                                                                 audio.data[self.start_sample:stop_sample])
                sample_duration_s = audio.sample_duration_s
                print(
                    f"maximum correlation found at {max_pos_samples} samples {sample_duration_s * max_pos_samples * 1000:1.0f} ms with value {corr:1.1f}")

                if max_pos_samples is not None:
                    if max_pos_samples > 0:
                        reference.aligned_data = reference.data[self.start_sample:]
                        audio.aligned_data = audio.data[self.start_sample + max_pos_samples:]
                    elif max_pos_samples < 0:
                        reference.aligned_data = reference.data[self.start_sample + max_pos_samples:]
                        audio.aligned_data = audio.data[self.start_sample:]
                    else:
                        reference.aligned_data = reference.data[self.start_sample:]
                        audio.aligned_data = audio.data[self.start_sample:]
                # min_len = min(len(reference.aligned_data), len(audio.aligned_data))
                # print(f"compare sizes of aligned data: reference : {len(reference.aligned_data)}, audio : {len(audio.aligned_data)}")
                # if len(audio.aligned_data) > min_len:
                #     audio.aligned_data = audio.aligned_data[:min_len]
                #     print("truncate audio aligned data")
                # elif len(reference.aligned_data) > min_len:
                #     reference.aligned_data = reference.aligned_data[:min_len]
                #     print("truncate reference aligned data")

    def get_silence_mask(self, audio_normalized_data):
        """
        Compute a mask of the silent frames in audio signal (normalized) as a numpy array of the same size with True
         for silent frames, False otherwise.
        :param audio_normalized_data: numpy array, normalized audio signal that is filtered.
        """

        # threshold
        silence_mask = np.zeros_like(audio_normalized_data, dtype=bool)
        if self.clean.sample_rate_hz == 48000:
            hw = 400
            th = 0.001
        else:
            hw = 200
            th = 0.001
        for i in range(audio_normalized_data.size):
            w0 = max(0, i - hw)
            wn = min(audio_normalized_data.size, i + hw + 1)
            if np.mean(np.abs(audio_normalized_data[w0:wn])) < th:
                silence_mask[i] = True

        # median filter on mask
        talk_hw = 1400
        silence_tmp = silence_mask.copy()
        for i in range(audio_normalized_data.size):
            w0 = max(0, i - talk_hw)
            wn = min(audio_normalized_data.size, i + talk_hw + 1)
            silence_tmp[i] = np.median(silence_mask[w0:wn])

        self.silence_mask = silence_tmp

    def detect_silence(self):
        """
        Detect the silence in speech and apply the silence mask to generate audio for silence and for talk
        for speech, noisy speech and clean audio.
        """

        if self.speech.data is None:
            self.silence_mask = np.ones_like(self.clean.data[self.start_sample:], dtype=bool)
            self.noisy_speech.silence = self.noisy_speech.data[self.start_sample:]
            self.clean.silence = self.clean.data[self.start_sample:]
            return

        # detect silence
        self.speech.normalize_aligned()
        self.get_silence_mask(self.speech.normalized_aligned_data)

        # truncate data
        n = min(min(self.noisy_speech.aligned_data.size, self.clean.aligned_data.size), len(self.silence_mask))
        self.speech.aligned_data = self.speech.aligned_data[:n]
        self.noisy_speech.aligned_data = self.noisy_speech.aligned_data[:n]
        self.clean.aligned_data = self.clean.aligned_data[:n]
        self.silence_mask = self.silence_mask[:n]

        # non-normalized
        self.speech.silence = self.speech.aligned_data[self.silence_mask]
        print(f"sizes: noisy_speech.aligned_data = {self.noisy_speech.aligned_data.size}, clean.aligned_data = {self.clean.aligned_data.size}")
        self.noisy_speech.silence = self.noisy_speech.aligned_data[self.silence_mask]
        self.clean.silence = self.clean.aligned_data[self.silence_mask]
        self.speech.talk = self.speech.aligned_data[np.logical_not(self.silence_mask)]
        self.noisy_speech.talk = self.noisy_speech.aligned_data[np.logical_not(self.silence_mask)]
        self.clean.talk = self.clean.aligned_data[np.logical_not(self.silence_mask)]

    def plot_audio_silence_and_talk(self):
        """
        Plot the compared audio from near-end and aec output, with the silence mask and the related audio for silence
        and talk.
        :return: plotly figure.
        """

        sample_duration_s = self.clean.sample_duration_s
        colors = ["#636EFA", "#00CC96", "#b00b69" ,"#EF553B"]
        if self.speech.aligned_data is not None:
            sig_max = max(np.max(self.speech.aligned_data), np.max(self.clean.aligned_data))
        else:
            sig_max = np.max(self.clean.aligned_data)
        fig_title = f'Silence and talk to compare, {self.test_suite_name}'
        signal_type = ["aligned", "silence", "talk"]
        audio_to_plot = {"speech": self.speech,
                         "clean": self.clean,
                         "noisy speech": self.noisy_speech}
        n_type = len(signal_type)
        n_audio = len(audio_to_plot.keys())
        fig = make_subplots(rows=n_audio * n_type, cols=1, shared_xaxes=True)

        for i, st in enumerate(signal_type):
            for j, sn in enumerate(audio_to_plot.keys()):

                audio_all = audio_to_plot[sn]
                if sn == "speech":
                    audio_all = self.speech
                elif sn == "clean":
                    audio_all = self.clean
                elif sn == "noisy speech":
                    audio_all = self.noisy_speech

                audio = None
                if st == "aligned":
                    audio = audio_all.aligned_data
                elif st == "silence":
                    audio = audio_all.silence
                elif st == "talk":
                    audio = audio_all.talk

                show_legend = True
                if i > 0:
                    show_legend = False

                if audio is not None:
                    # if sn == "noisy speech":
                    #     signal = audio
                    if sn == "silence":
                        signal = audio
                    else:
                        signal = audio / sig_max

                    timestamp = sample_duration_s * np.arange(signal.size)
                    fig.add_trace(go.Scatter(x=timestamp, y=signal, mode='lines', line={'width': 1, 'color': colors[j]},
                                             name=f"{sn}", showlegend=show_legend), row=i * n_audio + j + 1, col=1)
                    if st == "aligned":
                        show_mask_legend = True
                        if j > 0:
                            show_mask_legend = False
                        fig.add_trace(
                            go.Scatter(x=timestamp, y=0.7 * self.silence_mask, mode='lines',
                                       line={'width': 2, "color": colors[3]},
                                       name=f"mask", showlegend=show_mask_legend), row=i * n_audio + j + 1, col=1)
                fig.update_yaxes(title_text=f"{st}", range=[-1., 1.], row=i * n_audio + j + 1, col=1)
        fig.update_layout(title=fig_title)
        fig.update_xaxes(title_text='Time')
        fig.show()

        return fig

    def compute_SNR(self, noise):
        """
        Compute the Signal-To-Noise ratio between speech and noise in dB, on the range defined for aligned_data. This
        range is defined by [self.start_sample:N], where N is the length of the speech signal after the
        truncate_speech() step. An optional gain on noise can be applied.
        """

        print("--- compute SNR ---")
        print(f"truncated speech size = {self.speech.data.size}")
        print(f"aligned speech size = {self.speech.aligned_data.size}")
        print(f"start sample = {self.start_sample}")
        print(f"noise size = {noise.data.size}")

        speech_data = self.speech.aligned_data
        print(f"gain applied for noise = {self.noise_gain}")
        noise_data = self.noise_gain * noise.data[self.start_sample:]
        if noise_data.size < self.speech.aligned_data.size:
            speech_data = speech_data[:noise_data.size]
        else:
            noise_data = noise_data[:self.speech.aligned_data.size]
        print(f"noise data size = {noise_data.size}")
        print(f"speech data size = {speech_data.size}")

        speech_power = np.mean(speech_data ** 2)
        noise_power = np.mean(noise_data**2)
        self.snr_dB = 10. * np.log10(speech_power / noise_power)
        print(f"SNR = {self.snr_dB:0.2f} dB")
        print("---")

    def compute_energy_difference_with_speech(self):
        """
        Compute the energies of the silence parts of noisy audio and clean audio and their difference with speech audio.
        :return: The energies computed.
        """

        if self.speech.silence is None:
            energy_in_silence = np.sum(np.abs(self.clean.silence))
        else:
            print(f"energy measured in clean audio: {np.sum(np.abs(self.clean.silence)):1.1f} (= E_clean)")
            energy_in_silence = np.sum(np.abs(self.clean.silence)) - np.sum(
                np.abs(self.speech.silence))
        print(f"diff energy measured on silence: {energy_in_silence:1.1f}\t\t(= E_clean - E_speech)")

        if self.noisy_speech.silence is None:
            energy_in_silence_noisy_speech = np.sum(np.abs(self.noisy_speech.silence))
        else:
            energy_in_silence_noisy_speech = np.sum(np.abs(self.noisy_speech.silence)) - np.sum(
                np.abs(self.speech.silence))
        print(f"diff energy measured on silence of noisy audio: {energy_in_silence_noisy_speech:1.1f}\t(= E_noisy_speech - E_speech)")

        return energy_in_silence, energy_in_silence_noisy_speech

    def compute_acoustic_similarity(self, file_name_base):
        """
        Compute acoustic similarity between two audio signals based on MFCC coefficients.
        See https://librosa.org/doc/0.10.2/generated/librosa.feature.mfcc.html#librosa.feature.mfcc
        :return: matplotlib figure
        """

        ref_mfcc = mfcc_analysis(self.speech.talk, self.speech.sample_rate_hz)
        test_mfcc = mfcc_analysis(self.clean.talk, self.clean.sample_rate_hz)
        ref_mfcc.compute_mfcc()
        test_mfcc.compute_mfcc()
        test_mfcc.write_mfcc(file_name_base + "_test_MFCC_on_talk.npy")
        ref_mfcc.write_mfcc(file_name_base + "_ref_MFCC_on_talk.npy")
        # ref_mfcc.plot()
        # test_mfcc.plot()
        self.similarity_mfcc, self.distance_mfcc = test_mfcc.compute_similarity_with_reference(ref_mfcc.mfcc)
        fig = test_mfcc.plot()
        fig.savefig(file_name_base + "_MFCC_comparison_on_talk.png")
        test_mfcc.write_difference_with_ref(file_name_base + "_MFCC_difference_with_ref_on_talk.npy")

        # # if self.sampling_rate_hz <= 16000:
        # #     hop_length = 512
        # # else:
        # #     hop_length = 1024
        # # mfcc_ref = librosa.feature.mfcc(y=self.speech.talk, sr=self.speech.sample_rate_hz, hop_length=hop_length, htk=True)
        # # mfcc_test = librosa.feature.mfcc(y=self.clean.talk, sr=self.clean.sample_rate_hz, hop_length=hop_length,
        # #                                  htk=True)
        # mfcc_ref = librosa.feature.mfcc(y=self.speech.talk, sr=self.speech.sample_rate_hz)
        # mfcc_test = librosa.feature.mfcc(y=self.clean.talk, sr=self.clean.sample_rate_hz)
        #
        # # compute the mean of the differences for each coefficient
        # n_time = mfcc_ref.shape[1]
        # print(f"{mfcc_ref.shape[0]} coefs, {n_time} samples")
        # print(f"{mfcc_test.shape[0]} coefs, {mfcc_test.shape[1]} samples")
        # d = []
        # for t in range(n_time):
        #     d.append(np.linalg.norm(mfcc_ref[:, t] - mfcc_test[:, t]))
        # self.distance_mfcc = np.mean(d)
        #
        # if self.distance_mfcc == 0:
        #     self.similarity_mfcc = 1.
        # else:
        #     self.similarity_mfcc = 1. / self.distance_mfcc
        #
        # colors = plt.get_cmap("jet")
        # x_col = np.linspace(0, 1, mfcc_ref.shape[0])
        # diff_lim = [-200, 200]
        # n_rows = 4
        # fig, ax = plt.subplots(nrows=n_rows, ncols=1, sharex=True, figsize=(20, 15))
        #
        # # Set a common color scale for mfcc
        # v_min = np.min([mfcc_ref.min(), mfcc_test.min()])
        # v_max = np.max([mfcc_ref.max(), mfcc_test.max()])
        #
        # librosa.display.specshow(mfcc_ref, ax=ax[0], vmin=v_min, vmax=v_max)
        # ax[0].set_title('MFCC, reference')
        # ax[0].set_xlabel('Time')
        # ax[0].set_ylabel('Frequency range')
        #
        # librosa.display.specshow(mfcc_test, ax=ax[1], vmin=v_min, vmax=v_max)
        # ax[1].set_title('MFCC, test')
        # ax[1].set_xlabel('Time')
        # ax[1].set_ylabel('Frequency range')
        #
        # librosa.display.specshow(mfcc_ref - mfcc_test, ax=ax[2], vmin=diff_lim[0], vmax=diff_lim[1])
        # ax[2].set_title('Diff')
        # ax[2].set_xlabel('Time')
        # ax[2].set_ylabel('Frequency range')
        #
        # plt.subplot(n_rows, 1, 4)
        # plt.plot(d, label="instant distance")
        # for c in range(mfcc_ref.shape[0]):
        #     plt.plot(abs(mfcc_ref[c, :] - mfcc_test[c, :]), label=f"distance coef {c}", color=colors(x_col[c]))
        # plt.plot(d, label=f"mean distance", color="red", linewidth=3)
        # plt.ylim(0, diff_lim[1])
        #
        # # Adjust layout and display the plot
        # plt.tight_layout()
        # # plt.show(block=True)
        # plt.close()

    def compute_denoising_quality(self, log_file, start_analysis_time_ms=0, plot_silence_and_talk=True, noise=None):
        """
        Compute or load criteria to measure the quality of the denoising.
        :param log_file: full file name of the log file that contains the traces of the Noise Suppression test.
        :param start_analysis_time_ms: time stamp to start the audio analysis in clean and speech audio, in ms.
        Default is 0.
        :param plot_silence_and_talk: True whether the audio must be plotted with silence and talk parts. Default is
        True.
        """

        # load results in log file
        self.get_results(log_file)
        # self.print()

        if self.speech.sample_rate_hz != self.clean.sample_rate_hz:
            print(f"ERROR cannot compare speech and clean audio with different sampling rates")
            return

        if start_analysis_time_ms > 0:
            self.start_analysis_time_ms = start_analysis_time_ms
        print(f"\nstart_analysis_time_ms = {self.start_analysis_time_ms} ms, sample_rate_hz = {self.speech.sample_rate_hz} Hz")
        self.start_sample = int(float(self.start_analysis_time_ms * self.speech.sample_rate_hz) / 1000.)
        print(f"\nstart audio comparison at sample {self.start_sample} ({self.start_analysis_time_ms} ms)")

        # align audio if needed
        if self.clean.data is not None:

            # truncate the end of speech audio if the size is too different of clean audio
            self.truncate_speech()

            self.align_on_speech()

            # detect silence and talk
            self.detect_silence()
            if plot_silence_and_talk:
                self.plot_audio_silence_and_talk()

            # compute energy remaining in silence of noisy audio and clean audio
            self.compute_energy_difference_with_speech()

            # compute SNR if noise data given
            if noise.data is not None:
                self.compute_SNR(noise)

            # compare talk
            if self.speech.talk is not None:
                file_name_base = self.log_file.split(".log")[0]
                self.compute_acoustic_similarity(file_name_base)
                # fig.savefig(self.log_file.replace(".log", "_MFCC_comparison_on_talk.png"))
                # mfcc_file = self.log_file.replace(".log", "_MFCC_dist.txt")
                # with open(mfcc_file, "w") as f:
                #     for coef in d:
                #         f.write(f"{coef:0.6f}\n")

    def truncate_speech(self):
        """
        If the audio sizes are too different, the speech audio is shortened in order to reduce computation time for
        correlation. The beginning of the signal is kept, assuming that not all audio has been played in the test.
        """

        clean_size = self.clean.data.size
        k = 1.1
        if self.speech.data.size > k * self.clean.data.size:
            print(f"speech size is {self.speech.data.size}, too much compared with {clean_size}")
            new_size = int(k * clean_size)
            self.speech.data = self.speech.data[:new_size]
        else:
            print(f"speech size is {self.speech.data.size}, OK compared with {clean_size}")

        plot_to_check = False
        if plot_to_check:
            signal_name = ["speech", "noisy input", "clean output"]
            n_rows = len(signal_name)
            fig = make_subplots(rows=n_rows, cols=1, shared_xaxes=True)
            for i, audio in enumerate([self.speech, self.noisy_speech, self.clean]):
                audio_signal = audio.data
                if audio_signal is not None:
                    print(f"signal {signal_name[i]}, rate = {audio.sample_rate_hz} Hz")
                    signal = audio_signal / np.max(np.abs(audio_signal))
                    fig.add_trace(go.Scatter(x=audio.timestamps, y=signal, mode='lines', line={'width': 1},
                                             name=f"{signal_name[i]}"), row=i + 1, col=1)
                fig.update_yaxes(title_text=f'{signal_name[i]}', range=[-1., 1.], row=i + 1, col=1)
            fig.update_layout(title="truncated audio")
            fig.update_xaxes(title_text='Time')
            fig.show()
